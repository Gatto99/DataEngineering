# Tesi autore
Andrew NG, globalmente riconosciuto nel mondo dell'intelligenza artificiale, ha identificato un grande cambiamento in questo ambito: sostiene che l'uso dei modelli *data-centric*, per cui è sufficiente avere a disposizione pochi dati ma di qualità, si una soluzione migliore ad avere un dataset da scaricare usato per addestrare un modello di cui si pensa a migliorare il codice (*model-centric*). Inoltre ritiene che in alcune applicazioni, sia più produttivo mantenere lo stesso modello e migliorare la qualità dei dati. Quindi, secondo Andrew NG, collezionare sempre più dati risulta costoso e meno efficace che occuparsi dell'ingegnerizzazione dei dati. Infine, crede sia necessario che questo approccio diventi sistematico evitando rimanga una semplice intuizione di chi lavora in questo mondo. 

<!--## Foundation models
Sono modelli che vengono addestrati su una vasta quantità di dati e di cui si può fare un tuning per applicazioni specifiche. Ma non è banale verificare che siano equi e liberi da bias. Più in generale risultano efficiaci per alcuni problemi, ma non per altri. 

Inoltre, alcuni domini applicativi sollevano problemi di scalabilità perché se dovessimo costruire dei foundation models per la computer vision, si dovrebbero processare una gran quantità di immagini. -->

L'utilizzo dei foundation models può risultare vantaggioso qualora si disponesse di poche risorse da applicare per risolvere un determinato problema, quindi si utilizzano questi modelli per applicarli successivamente a task differenti. In questo caso però, ci si può scontrare con una mancata specializzazione del modello al dominio applicativo in questione, in questo senso sarebbe meglio, se possibile, sfruttare modelli che, con pochi dati e di qualità, riescano a raggiungere l'obiettivo. Questo richiede inevitabilmente che ci siano degli operatori, che lo stesso Andrew NG chiama MLOps, che devono curarsi della qualità dei dati piuttosto che del codice e di conseguenza questo richiede un impiego di risorse per ogni specifico problema.

<!--## Da big data a good data 
Migliorare il modello (e dunque il codice), è stato il paradigma con cui i problemi sono stati via via risolti, ma per alcuni domini applicativi, è più produttivo migliorare i dati. 
Questo cambiamento risulta fondamentale ed utile per quelle aziende che non hanno a disposizione un grande data sets su cui addestrare i modelli, per cui averne pochi ma attentamente progettati può essere sufficiente. 

In questo caso si parla di prendere dei modelli già addestrati che potranno essere rifiniti attraverso la scelta di un giusto sottoinsieme di dati che è permessa dall'uso di strumenti idonei che permettono di etichettare queste immagini consistentemente. Invece se utilizzassimo solo tanti dati che poi risultano rumorosi, allora si ignora il problema e si collezionano più dati per ricoprirli. Ma è più efficiente ricoprire questi casi dando in pasto al modello dei dati specifici che permettono di risolvere l'inconsistenza dovuta al rumore. Se il bias è per un certo sottoinsieme, cambiare tutta l'archietettura per eliminarlo, è difficile. Quindi è necessario avere strumenti che permettano di accorgersi che il bias sia relativo ad un subset dei dati così da attirare l'attenzione. 
Collezionare tanti dati per tutto può essere costoso

Questi modelli data-centric non possono essere la soluzione assoluta, ma devono essere accompagnati da strumenti che danno la possibilià alle aziende, di avere strumenti per ingegnerizzare i dati e sfruttare la loro conoscenza del dominio per costruire il modello adatto.-->

Credo che sia furbo ed efficace risolvere i problemi specifici cercando di dare in pasto al modello dati che permettano di risolvere direttamente l'inconsistenza dovuta ad un errata etichetta associata a determinati dati. Piuttosto che continuare a raccogliere dati che potrebbe richiedere di prendere dati anche non esistente, potrebbe comunque non risolvere un problema direttamente ma farlo in senso lato ci può far peredere tempo e risorse che potremmo impiegare a migliorare dati che già abbiamo. Come suggerito da NG questo approccio risulta utile a quelle aziende che non dispongono di grandi data sets, infatti queste realtà esistono. Se ci si dovesse domandare cosa fosse meglio, se migliorare il modello o i dati, il mio suggerimento sarebbe quello di migliorare i dati poiché se i dati sono gli stessi, migliorare il modello potrebbe si risolvere il problema, ma magari applico un modello che per sopportare la fatica nell soddisfare l'obiettivo deve impiegare più risorse e quindi risulta non efficiente come vorremmo. Invece migliorare i dati permette di applicare allo stesso dominio, differenti modelli che possono essere più o meno efficienti.

<!--## Dati sintetici
Potrebbero essere utili ma userebbe metodologie che risultano più facili come il data augmentation che prevede di applicare trasformazioni reali a dati per ottenerne degli altri ingrandendo il data set a disposizione.-->